# {{ name }} - Context-Aware Assistant (L2)

An intelligent assistant that **remembers conversations** using Lakebase (Databricks PostgreSQL).

## What You'll Learn (L2)

âœ… **Conversation Memory** - Store and retrieve chat history  
âœ… **Lakebase Integration** - Use managed PostgreSQL for state  
âœ… **Session Management** - Track conversations by user/session  
âœ… **Context-Aware Responses** - Reference past conversations  
âœ… **RAG (Optional)** - Knowledge retrieval from documents  
âœ… **Vector Search** - pgvector or Databricks Vector Search  
âœ… **MLflow Tracing** - Track all interactions

---

## Quick Start

### 1. Set Up Lakebase

Create a Lakebase instance in Databricks:
1. Go to **Compute** â†’ **Lakebase**
2. Click **Create Instance**
3. Note your connection details:
   - Host: `your-instance.lakebase.databricks.com`
   - Database: `agents`
   - User: `your-user`
   - Password: `your-password`

See [Databricks Lakebase docs](https://docs.databricks.com/lakebase/) for details.

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Set Environment Variables

```bash
# Databricks
export DATABRICKS_HOST="https://your-workspace.cloud.databricks.com"
export DATABRICKS_TOKEN="your-token"

# Lakebase
export LAKEBASE_HOST="your-instance.lakebase.databricks.com"
export LAKEBASE_DATABASE="agents"
export LAKEBASE_USER="your-user"
export LAKEBASE_PASSWORD="your-password"
```

### 4. Run Locally

**CLI Version:**
```bash
python assistant.py
```

**Web Version:**
```bash
python app.py
# Visit http://localhost:8000
```

---

## How It Works

### Memory + RAG Flow

```
User Message
    â†“
Store in Lakebase (PostgreSQL)
    â†“
Retrieve conversation history (last 20 messages)
    â†“
[If RAG enabled] Retrieve relevant documents (pgvector/Vector Search)
    â†“
Combine: history + documents + user message
    â†“
Send to LLM with full context
    â†“
Store LLM response in Lakebase
    â†“
Return to user
```

### Session Management

Each conversation has a unique `session_id`:
- **CLI:** Generated on start, reused across the session
- **Web:** Each browser session gets a unique ID
- **API:** Pass `session_id` in requests

### Memory Architecture

```python
# Lakebase stores conversations in PostgreSQL
conversations
â”œâ”€â”€ id (serial)
â”œâ”€â”€ session_id (varchar)  # Unique per user/session
â”œâ”€â”€ role (varchar)         # "user" or "assistant"
â”œâ”€â”€ content (text)         # Message content
â”œâ”€â”€ timestamp (timestamp)  # When message was sent
â””â”€â”€ metadata (jsonb)       # Optional metadata
```

Reference: [Databricks Stateful Agents](https://docs.databricks.com/aws/en/generative-ai/agent-framework/stateful-agents)

---

## Adding RAG (Optional)

### What is RAG?

**RAG (Retrieval-Augmented Generation)** lets your assistant answer questions using your own documents/knowledge base, not just conversation history.

### When to Use RAG

- **Without RAG:** Simple conversational assistant (remembers chat history)
- **With RAG:** Knowledge-powered assistant (chat history + document search)

### Enable RAG in 3 Steps

#### Step 1: Prepare Documents

Put your documents in a UC Volume:
```bash
# In Databricks workspace
dbfs:/Volumes/main/docs/source/
â”œâ”€â”€ product_manual.pdf
â”œâ”€â”€ faq.md
â”œâ”€â”€ technical_docs.txt
â””â”€â”€ ... more docs
```

Supported formats: `.txt`, `.md`, `.pdf`, `.docx`, `.html`

#### Step 2: Enable RAG in `config.yaml`

```yaml
rag:
  enabled: true
  source: /Volumes/main/docs/source  # Your UC Volume path
  backend: pgvector  # or vector_search
  top_k: 5  # Number of docs to retrieve per query
```

#### Step 3: Run

```bash
python app.py
# On first run:
# ðŸ”§ Setting up RAG with pgvector...
# âœ“ pgvector extension enabled
# âœ“ Embeddings table ready
# âœ“ Vector index created
# ðŸ“„ Checking documents in /Volumes/main/docs/source...
# ðŸ”„ Indexing 42 new documents...
# âœ… Indexed 42 documents
# âœ… RAG ready (pgvector)!
```

**That's it!** Your assistant now uses documents to answer questions.

### pgvector vs Vector Search

| Feature | pgvector (Default) | Databricks Vector Search |
|---------|-------------------|-------------------------|
| **Best For** | Small-medium knowledge bases | Enterprise-scale |
| **Documents** | Thousands | Millions |
| **Setup** | Auto (1 config line) | Manual index creation |
| **Cost** | Included in Lakebase | Separate endpoints |
| **Location** | Co-located with memory | Separate service |

**Recommendation:** Start with `pgvector`, switch to `vector_search` if you need scale.

### Switching to Vector Search

```yaml
rag:
  enabled: true
  source: /Volumes/main/docs/source
  backend: vector_search  # Changed!
  vector_search_endpoint: one-env-shared-endpoint-0  # Your endpoint
  index_name: main.default.docs_index  # catalog.schema.index_name
  embedding_model: databricks-bge-large-en  # For auto-embedding
```

**What happens on first run:**
1. âœ… Creates Delta table: `main.default.docs_index_source`
2. âœ… Creates Vector Search index on the Delta table (TRIGGERED mode)
3. âœ… Indexes documents from UC Volume to Delta table
4. âœ… Vector Search auto-generates embeddings
5. âœ… Sync is triggered automatically after indexing (cost-effective)

**Prerequisites:**
- SQL Warehouse (for Delta table operations)
- Vector Search endpoint (create in Databricks UI if needed)
- Permissions to create tables in the specified catalog/schema

You'll need to manually create the Vector Search index:
```sql
-- In Databricks SQL
CREATE TABLE main.docs.source_table AS
SELECT * FROM read_files('/Volumes/main/docs/source');

-- Create Vector Search index (via UI or API)
```

See [Databricks Vector Search docs](https://docs.databricks.com/vector-search/).

### How RAG Works

```
User: "How do I deploy to production?"
    â†“
1. Get conversation history (Lakebase)
2. Retrieve relevant docs (pgvector/Vector Search)
    - "deployment_guide.md" (similarity: 0.92)
    - "production_checklist.md" (similarity: 0.88)
    - ...
3. Combine history + retrieved docs
4. Send to LLM with full context
5. LLM answers using your docs âœ¨
```

### Keeping Documents Current

**Option 1: Manual Refresh (Simple)**
```bash
# CLI: Type 'refresh' during conversation
> refresh

# Or restart the app
python app.py
```

**Option 2: Scheduled Workflow (Recommended for Production)**

Create a Databricks Workflow to sync nightly:

```python
# rag_sync_job.py
from databricks_agent_toolkit.integrations import Lakebase
from {{ name }}.rag_manager import RAGManager
import yaml

# Load config
with open("config.yaml") as f:
    config = yaml.safe_load(f)

# Initialize RAG manager
if config["rag"]["backend"] == "pgvector":
    lakebase = Lakebase()
    rag = RAGManager(config["rag"], lakebase_client=lakebase)
else:
    rag = RAGManager(config["rag"])

# Refresh index
rag.refresh()
print("âœ… RAG index refreshed")
```

**Workflow Configuration:**
```yaml
# In Databricks Workflows UI or bundle.yml
resources:
  jobs:
    rag_sync_job:
      name: "{{ name }} - RAG Sync"
      schedule:
        quartz_cron_expression: "0 0 2 * * ?"  # Daily at 2 AM
        timezone_id: "America/Los_Angeles"
      tasks:
        - task_key: sync_rag
          python_file: rag_sync_job.py
          libraries:
            - pypi:
                package: databricks-agent-toolkit>=0.1.3
```

**Why Workflows?**
- âœ… Runs independently of the app
- âœ… No impact on app performance
- âœ… Scheduled (nightly, hourly, etc.)
- âœ… Monitored via Databricks Jobs UI
- âœ… Scalable (dedicated cluster)

---

## Configuration

Edit `config.yaml` to customize:

### Model Settings

```yaml
model:
  endpoint: databricks-claude-sonnet-4-5
  streaming: false  # Enable for token-by-token
  system_prompt: |
    You are a helpful assistant...
```

**Popular Models:**
- `databricks-claude-sonnet-4-5` (recommended)
- `databricks-gpt-5`
- `databricks-gemini-2-5-pro`
- `databricks-llama-4-maverick`

### Memory Settings

```yaml
memory:
  max_history: 20  # Number of messages in context window
```

**Trade-offs:**
- **More history** = Better context, higher latency, more tokens
- **Less history** = Faster, cheaper, but may forget earlier context

### RAG Settings

```yaml
rag:
  enabled: true
  source: /Volumes/main/docs/source  # UC Volume path
  backend: pgvector  # or vector_search
  top_k: 5  # Documents to retrieve
  chunk_size: 500  # Characters per chunk
  embedding_model: databricks-bge-large-en
  index_type: ivfflat  # or hnsw (pgvector only)
```

**Tuning:**
- `top_k`: More = better context, slower, more tokens
- `chunk_size`: Larger = more context per chunk, fewer chunks
- `backend`: pgvector (simple) vs vector_search (scale)
- `index_type`: **IVFFlat** (fast updates, 90-95% recall) vs **HNSW** (better accuracy, 95-99% recall, slower indexing)

**Index Type Guidance (pgvector only):**

| Use Case | Recommended Index |
|----------|------------------|
| Documents updated frequently | `ivfflat` (default) |
| Read-heavy, static knowledge base | `hnsw` |
| Need highest accuracy | `hnsw` |
| Large datasets (>50k docs) | `ivfflat` |
| Small, critical dataset | `hnsw` |

---

## Deploy to Databricks Apps

### 1. Set Up Service Principal

```bash
export DATABRICKS_CLIENT_ID="your-sp-client-id"
export DATABRICKS_CLIENT_SECRET="your-sp-client-secret"
```

### 2. Configure Lakebase in `databricks-app.yml`

```yaml
env:
  - name: LAKEBASE_HOST
    value: "your-instance.lakebase.databricks.com"
  - name: LAKEBASE_DATABASE
    value: "agents"
  - name: LAKEBASE_USER
    value: "your-user"
  - name: LAKEBASE_PASSWORD
    value: "your-password"  # Use secrets in production!
```

### 3. Deploy

```bash
databricks apps deploy {{ name }} \
    --source-code-path . \
    --app-config-file databricks-app.yml
```

### 4. Add Permissions

In Databricks UI:
1. Go to **Apps** â†’ **{{ name }}**
2. **Resources** â†’ **Add Resource**
3. Add **Model Serving** with **Can query** permission

---

## Advanced: Short-term vs. Long-term Memory

This scaffold implements **short-term memory** (conversation history).

For **long-term memory** (persistent facts across sessions):
- See [Databricks stateful agents docs](https://docs.databricks.com/aws/en/generative-ai/agent-framework/stateful-agents)
- Use LangGraph checkpointing for "time travel"
- Store user preferences in separate tables
- Implement fact extraction and summarization

---

## Troubleshooting

### "Could not connect to Lakebase"
- Verify `LAKEBASE_HOST`, `LAKEBASE_DATABASE`, `LAKEBASE_USER`, `LAKEBASE_PASSWORD`
- Check Lakebase instance is running
- Ensure network connectivity (VPC peering if needed)

### "Table does not exist"
- The assistant auto-creates the `conversations` table on first run
- If you see errors, manually create it:
  ```sql
  CREATE TABLE conversations (
      id SERIAL PRIMARY KEY,
      session_id VARCHAR(255) NOT NULL,
      role VARCHAR(50) NOT NULL,
      content TEXT NOT NULL,
      timestamp TIMESTAMP DEFAULT NOW(),
      metadata JSONB
  );
  CREATE INDEX idx_session ON conversations(session_id);
  ```

### "Context window exceeded"
- Reduce `memory.max_history` in `config.yaml`
- Reduce `rag.top_k` if RAG enabled
- Or implement conversation summarization

### "RAG setup failed" / "pgvector extension not found"
- Ensure Lakebase instance supports extensions
- Check Lakebase version (pgvector requires PostgreSQL 11+)
- Try restarting Lakebase instance

### "No documents indexed"
- Verify UC Volume path is correct and accessible
- Check file formats are supported (.txt, .md, .pdf, .docx, .html)
- Ensure Databricks workspace has access to UC Volume
- Try manual refresh: Type `refresh` in CLI or call `/api/rag/refresh` endpoint

### "Embedding generation failed"
- Verify `DATABRICKS_HOST` and `DATABRICKS_TOKEN` are set
- Check embedding model endpoint is accessible
- Default model: `databricks-bge-large-en` (ensure it's deployed)

---

## Next Steps

ðŸŽ¯ **Level Up to L3 (API)**
- Expose as production API
- Add authentication & rate limiting
- Health checks & monitoring

ðŸ“š **Learn More:**
- [Databricks Lakebase](https://docs.databricks.com/lakebase/)
- [Stateful Agents](https://docs.databricks.com/aws/en/generative-ai/agent-framework/stateful-agents)
- [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html)

---

**Built with:** [Databricks Agent Toolkit](https://pypi.org/project/databricks-agent-toolkit/)
