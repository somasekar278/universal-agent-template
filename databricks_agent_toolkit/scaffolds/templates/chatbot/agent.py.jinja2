"""
{{ name }} - Chatbot Agent (OpenAI API Compatible)
Generated by: Databricks Agent Toolkit v0.2.2

Uses MLflow AgentServer with @invoke/@stream decorators.
"""
import yaml
import asyncio
import uuid
from pathlib import Path
from typing import List, Dict
import mlflow
from mlflow.deployments import get_deploy_client
from mlflow.genai.agent_server.server import invoke, stream
from mlflow.types.responses import ResponsesAgentRequest, ResponsesAgentResponse

# Load config
config = yaml.safe_load((Path(__file__).parent / "config.yaml").read_text())

# Setup MLflow
mlflow.set_tracking_uri("databricks")
try:
    mlflow.set_experiment(config["mlflow"]["experiment"])
    print(f"ðŸ“Š MLflow: {config['mlflow']['experiment']}")
except Exception as e:
    print(f"âš ï¸  MLflow setup: {e}")

# Initialize LLM client
llm_client = get_deploy_client("databricks")
endpoint = config["model"]["endpoint"]
temperature = config["model"].get("temperature", 0.7)
max_tokens = config["model"].get("max_tokens", 500)
system_prompt = config["model"].get("system_prompt", "You are a helpful AI assistant.")
stream_delay_ms = config["model"].get("token_delay_ms", 50) / 1000.0

print(f"ðŸ¤– Agent initialized: {endpoint}")


@invoke()
async def invoke_handler(request) -> ResponsesAgentResponse:
    """Non-streaming invocation handler."""
    # Extract messages (handles both dict and ResponsesAgentRequest)
    input_messages = request.get("input", []) if isinstance(request, dict) else request.input

    messages = [{"role": "system", "content": system_prompt}]
    for msg in input_messages:
        if isinstance(msg, dict):
            messages.append(msg)
        elif hasattr(msg, 'model_dump'):
            messages.append(msg.model_dump())
        else:
            messages.append({"role": msg.role, "content": msg.content})

    # Call LLM
    response = llm_client.predict(
        endpoint=endpoint,
        inputs={"messages": messages, "temperature": temperature, "max_tokens": max_tokens}
    )

    # Return ResponsesAgent format
    content = response["choices"][0]["message"]["content"]
    return ResponsesAgentResponse(
        output=[{
            "id": str(uuid.uuid4()),
            "type": "message",
            "role": "assistant",
            "content": [{"type": "output_text", "text": content}]
        }]
    )


@stream()
async def stream_handler(request: dict):
    """Streaming invocation handler."""
    # Extract messages from request
    input_messages = request.get("input", [])
    messages = [{"role": "system", "content": system_prompt}]
    messages.extend(input_messages)

    # Call LLM (non-streaming, then simulate streaming for demo)
    response = llm_client.predict(
        endpoint=endpoint,
        inputs={"messages": messages, "temperature": temperature, "max_tokens": max_tokens}
    )
    content = response["choices"][0]["message"]["content"]

    # Stream with configurable delay
    for word in content.split():
        yield {"role": "assistant", "content": word + " "}
        await asyncio.sleep(stream_delay_ms)
