# ðŸš€ Deployment Guide - SOTA Agent Framework

**Complete guide to deploying agent solutions built with SOTA Agent Framework.**

---

## ðŸ“‹ Table of Contents

1. [Deployment Overview](#deployment-overview)
2. [Pre-Deployment Checklist](#pre-deployment-checklist)
3. [Deployment Options](#deployment-options)
4. [Docker Deployment](#docker-deployment)
5. [Kubernetes Deployment](#kubernetes-deployment)
6. [Databricks Deployment](#databricks-deployment)
7. [Serverless Deployment](#serverless-deployment)
8. [CI/CD Pipelines](#cicd-pipelines)
9. [Configuration Management](#configuration-management)
10. [Monitoring & Observability](#monitoring--observability)
11. [Scaling Strategy](#scaling-strategy)
12. [Production Best Practices](#production-best-practices)

---

## ðŸŽ¯ Deployment Overview

### What Gets Deployed?

When you deploy an agent solution built with this framework, you're deploying:

```
Your Agent Solution
â”œâ”€â”€ Agent Code (your custom agents)
â”œâ”€â”€ Configuration (YAML configs)
â”œâ”€â”€ Dependencies (framework + your deps)
â”œâ”€â”€ API Layer (FastAPI/WebSocket)
â”œâ”€â”€ Background Workers (async processing)
â””â”€â”€ Monitoring (telemetry, metrics)
```

### Deployment Modes

| Mode | Best For | Complexity | Cost |
|------|----------|------------|------|
| **Docker** | Single service, any cloud | Low | $ |
| **Kubernetes** | Multiple services, enterprise | Medium | $$ |
| **Databricks** | Data-intensive, ML workloads | Medium | $$$ |
| **Serverless** | Event-driven, variable load | Low | $ (pay-per-use) |

---

## âœ… Pre-Deployment Checklist

### 1. **Package Your Agent Solution**

```bash
# Your agent project structure
my-agent-solution/
â”œâ”€â”€ agents/              # Your custom agents
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ my_agent.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ sota_config.yaml
â”‚   â””â”€â”€ agents.yaml
â”œâ”€â”€ requirements.txt     # Your dependencies
â”œâ”€â”€ Dockerfile          # Generated by framework
â”œâ”€â”€ pyproject.toml      # Package metadata
â””â”€â”€ main.py            # Entry point
```

### 2. **Test Locally**

```bash
# Install dependencies
pip install -r requirements.txt

# Run tests
pytest tests/

# Test API locally
python -m services.api
curl http://localhost:8000/health
```

### 3. **Configure for Production**

```yaml
# config/sota_config.yaml
environment: production

execution:
  default_mode: "process_pool"
  pool_size: 16

monitoring:
  enabled: true
  telemetry_endpoint: "${TELEMETRY_ENDPOINT}"
  
databricks:
  host: "${DATABRICKS_HOST}"
  token: "${DATABRICKS_TOKEN}"
```

### 4. **Security Checklist**

- [ ] Secrets in environment variables (not code)
- [ ] API authentication enabled
- [ ] HTTPS/TLS configured
- [ ] Network policies defined
- [ ] Rate limiting enabled
- [ ] Input validation implemented

---

## ðŸ³ Docker Deployment

### Option 1: Generated Dockerfile

The framework auto-generates a production-ready Dockerfile:

```dockerfile
# Generated by sota-generate
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Install your agent solution
RUN pip install --no-cache-dir -e .

# Expose ports
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# Run API server
CMD ["python", "-m", "uvicorn", "services.api:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Option 2: Multi-Stage Build (Optimized)

```dockerfile
# Stage 1: Builder
FROM python:3.11-slim as builder

WORKDIR /build
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# Stage 2: Runtime
FROM python:3.11-slim

WORKDIR /app

# Copy dependencies from builder
COPY --from=builder /root/.local /root/.local
ENV PATH=/root/.local/bin:$PATH

# Copy application
COPY . .

# Non-root user for security
RUN useradd -m -u 1000 agentuser && chown -R agentuser:agentuser /app
USER agentuser

EXPOSE 8000
HEALTHCHECK --interval=30s CMD python -c "import requests; requests.get('http://localhost:8000/health')"

CMD ["python", "-m", "uvicorn", "services.api:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### Build & Run

```bash
# Build image
docker build -t my-agent-solution:v1.0.0 .

# Run locally
docker run -d \
  -p 8000:8000 \
  -e DATABRICKS_TOKEN=${DATABRICKS_TOKEN} \
  -e DATABRICKS_HOST=${DATABRICKS_HOST} \
  --name my-agent \
  my-agent-solution:v1.0.0

# Check health
curl http://localhost:8000/health

# View logs
docker logs -f my-agent
```

### Docker Compose (Multi-Service)

```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=production
      - DATABRICKS_TOKEN=${DATABRICKS_TOKEN}
      - DATABRICKS_HOST=${DATABRICKS_HOST}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
  
  worker:
    build: .
    command: python -m services.worker
    environment:
      - ENVIRONMENT=production
      - DATABRICKS_TOKEN=${DATABRICKS_TOKEN}
    depends_on:
      - api
    restart: unless-stopped
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    restart: unless-stopped
```

```bash
# Deploy with compose
docker-compose up -d

# Scale workers
docker-compose up -d --scale worker=5

# View logs
docker-compose logs -f api
```

---

## â˜¸ï¸ Kubernetes Deployment

### Generated K8s Manifests

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-agent-solution
  labels:
    app: my-agent
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-agent
  template:
    metadata:
      labels:
        app: my-agent
    spec:
      containers:
      - name: agent-api
        image: my-agent-solution:v1.0.0
        ports:
        - containerPort: 8000
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: DATABRICKS_TOKEN
          valueFrom:
            secretKeyRef:
              name: databricks-secret
              key: token
        - name: DATABRICKS_HOST
          valueFrom:
            configMapKeyRef:
              name: databricks-config
              key: host
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: my-agent-service
spec:
  selector:
    app: my-agent
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-agent-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-agent-solution
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### Deploy to Kubernetes

```bash
# Create namespace
kubectl create namespace agent-production

# Create secrets
kubectl create secret generic databricks-secret \
  --from-literal=token=${DATABRICKS_TOKEN} \
  -n agent-production

# Create configmap
kubectl create configmap databricks-config \
  --from-literal=host=${DATABRICKS_HOST} \
  -n agent-production

# Deploy
kubectl apply -f k8s/ -n agent-production

# Check status
kubectl get pods -n agent-production
kubectl get svc -n agent-production

# View logs
kubectl logs -f deployment/my-agent-solution -n agent-production

# Scale manually
kubectl scale deployment my-agent-solution --replicas=10 -n agent-production
```

---

## ðŸ§± Databricks Deployment

### Option 1: Databricks Jobs (Batch Processing)

```python
# databricks_job.py
from databricks.sdk import WorkspaceClient

w = WorkspaceClient()

job = w.jobs.create(
    name="fraud-detection-agent",
    tasks=[
        {
            "task_key": "process_transactions",
            "description": "Process transactions with agents",
            "new_cluster": {
                "spark_version": "13.3.x-scala2.12",
                "node_type_id": "i3.xlarge",
                "num_workers": 4
            },
            "python_wheel_task": {
                "package_name": "my_agent_solution",
                "entry_point": "main",
                "parameters": ["--date", "{{job.start_time}}"]
            },
            "libraries": [
                {"pypi": {"package": "sota-agent-framework[all]"}},
                {"whl": "dbfs:/wheels/my_agent_solution-1.0.0-py3-none-any.whl"}
            ]
        }
    ],
    schedule={
        "quartz_cron_expression": "0 0 * * * ?",  # Every hour
        "timezone_id": "America/Los_Angeles"
    }
)

print(f"Job created: {job.job_id}")
```

### Option 2: Databricks Model Serving (Real-time)

```yaml
# databricks_serving.yaml
name: fraud-agent-endpoint
config:
  served_models:
  - model_name: fraud_detection_agent
    model_version: "1"
    workload_size: Small
    scale_to_zero_enabled: true
    environment_vars:
      SOTA_CONFIG: "dbfs:/configs/sota_config.yaml"
```

```bash
# Deploy with Databricks CLI
databricks serving-endpoints create --config databricks_serving.yaml
```

### Option 3: Databricks Workflows

```python
# workflows/fraud_detection.py
from databricks.sdk import WorkspaceClient
from databricks.sdk.service import jobs

w = WorkspaceClient()

# Create workflow
workflow = w.jobs.create(
    name="fraud-detection-workflow",
    tasks=[
        jobs.Task(
            task_key="extract",
            notebook_task=jobs.NotebookTask(
                notebook_path="/Workspace/fraud/extract",
                base_parameters={"date": "{{job.start_time}}"}
            ),
            new_cluster=jobs.ClusterSpec(
                spark_version="13.3.x-scala2.12",
                node_type_id="i3.xlarge",
                num_workers=2
            )
        ),
        jobs.Task(
            task_key="agent_processing",
            depends_on=[jobs.TaskDependency(task_key="extract")],
            python_wheel_task=jobs.PythonWheelTask(
                package_name="my_agent_solution",
                entry_point="process_batch"
            ),
            new_cluster=jobs.ClusterSpec(
                spark_version="13.3.x-scala2.12",
                node_type_id="i3.xlarge",
                num_workers=4
            ),
            libraries=[
                jobs.Library(pypi=jobs.PythonPyPiLibrary(package="sota-agent-framework[databricks]")),
                jobs.Library(whl="dbfs:/wheels/my_agent_solution-1.0.0-py3-none-any.whl")
            ]
        ),
        jobs.Task(
            task_key="load_results",
            depends_on=[jobs.TaskDependency(task_key="agent_processing")],
            spark_python_task=jobs.SparkPythonTask(
                python_file="dbfs:/scripts/load_results.py"
            )
        )
    ]
)

print(f"Workflow created: {workflow.job_id}")
```

---

## âš¡ Serverless Deployment

### AWS Lambda

```python
# lambda_handler.py
import json
from agents import AgentRouter
from shared.schemas import AgentInput

# Initialize once (cold start)
router = AgentRouter.from_yaml("config/agents.yaml")

def lambda_handler(event, context):
    """AWS Lambda handler for agent execution."""
    
    try:
        # Parse request
        body = json.loads(event['body'])
        
        # Execute agent
        request = AgentInput(**body)
        result = await router.route(body['agent_name'], request)
        
        return {
            'statusCode': 200,
            'body': json.dumps(result.dict())
        }
    
    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }
```

```yaml
# serverless.yml
service: my-agent-solution

provider:
  name: aws
  runtime: python3.11
  region: us-west-2
  memorySize: 1024
  timeout: 300
  environment:
    DATABRICKS_TOKEN: ${env:DATABRICKS_TOKEN}
    DATABRICKS_HOST: ${env:DATABRICKS_HOST}

functions:
  executeAgent:
    handler: lambda_handler.lambda_handler
    events:
      - http:
          path: /execute
          method: post
    layers:
      - arn:aws:lambda:us-west-2:123456789:layer:sota-agent-framework:1

package:
  patterns:
    - '!tests/**'
    - '!docs/**'
    - '!.git/**'
```

```bash
# Deploy with Serverless Framework
serverless deploy --stage production
```

### Google Cloud Functions

```python
# main.py
import functions_framework
from agents import AgentRouter

router = AgentRouter.from_yaml("config/agents.yaml")

@functions_framework.http
def execute_agent(request):
    """Cloud Function entry point."""
    request_json = request.get_json()
    
    result = await router.route(
        request_json['agent_name'],
        AgentInput(**request_json)
    )
    
    return result.dict()
```

```bash
# Deploy
gcloud functions deploy execute-agent \
  --runtime python311 \
  --trigger-http \
  --entry-point execute_agent \
  --memory 1024MB \
  --timeout 300s \
  --set-env-vars DATABRICKS_TOKEN=${DATABRICKS_TOKEN}
```

---

## ðŸ”„ CI/CD Pipelines

### GitHub Actions

```yaml
# .github/workflows/deploy.yml
name: Deploy Agent Solution

on:
  push:
    branches: [main]
    tags: ['v*']

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Run tests
        run: pytest tests/ -v
      
      - name: Run framework validation
        run: python test_framework.py --quick
  
  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Docker Build
        uses: docker/setup-buildx-action@v2
      
      - name: Login to Container Registry
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Build and push
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: |
            ghcr.io/${{ github.repository }}:latest
            ghcr.io/${{ github.repository }}:${{ github.sha }}
  
  deploy-k8s:
    needs: build
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    steps:
      - uses: actions/checkout@v3
      
      - name: Configure kubectl
        uses: azure/k8s-set-context@v3
        with:
          kubeconfig: ${{ secrets.KUBE_CONFIG }}
      
      - name: Deploy to Kubernetes
        run: |
          kubectl set image deployment/my-agent \
            agent-api=ghcr.io/${{ github.repository }}:${{ github.sha }} \
            -n production
          
          kubectl rollout status deployment/my-agent -n production
  
  deploy-databricks:
    needs: test
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Databricks CLI
        run: pip install databricks-cli
      
      - name: Build wheel
        run: python -m build
      
      - name: Upload to Databricks
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          databricks fs cp dist/*.whl dbfs:/wheels/ --overwrite
          databricks jobs run-now --job-id ${{ secrets.JOB_ID }}
```

---

## âš™ï¸ Configuration Management

### Environment-Specific Configs

```yaml
# config/sota_config.yaml (base)
execution:
  default_mode: "process_pool"

monitoring:
  enabled: true
```

```yaml
# config/sota_config.production.yaml (override)
execution:
  pool_size: 32

monitoring:
  telemetry_endpoint: "https://telemetry.prod.company.com"
  sample_rate: 0.1

databricks:
  host: "${DATABRICKS_PROD_HOST}"
  token: "${DATABRICKS_PROD_TOKEN}"

rate_limiting:
  enabled: true
  requests_per_minute: 1000
```

### Loading Config

```python
# main.py
import os
from sota_agent import load_config

# Load environment-specific config
env = os.getenv("ENVIRONMENT", "development")
config = load_config(f"config/sota_config.{env}.yaml")
```

---

## ðŸ“Š Monitoring & Observability

### Built-in Telemetry

```python
# Automatically enabled in production
from telemetry import AgentTracer

tracer = AgentTracer()

@tracer.trace_agent("fraud_detection")
async def process_transaction(txn):
    # Automatically traced:
    # - Execution time
    # - Input/output
    # - Errors
    # - Resource usage
    pass
```

### Metrics Dashboard

```python
# monitoring/dashboard.py
from monitoring import MetricsCollector

metrics = MetricsCollector()

# Automatic metrics:
# - agent_executions_total
# - agent_execution_duration_seconds
# - agent_errors_total
# - agent_queue_depth
# - memory_usage_bytes
```

### Databricks Monitoring

```sql
-- Query agent execution metrics
SELECT 
  agent_name,
  COUNT(*) as executions,
  AVG(execution_time_ms) as avg_time_ms,
  PERCENTILE(execution_time_ms, 0.95) as p95_time_ms,
  SUM(CASE WHEN success = false THEN 1 ELSE 0 END) as errors
FROM agent_telemetry
WHERE date >= CURRENT_DATE - INTERVAL 1 DAY
GROUP BY agent_name
```

---

## ðŸ“ˆ Scaling Strategy

### Horizontal Scaling

```yaml
# K8s HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-agent-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-agent-solution
  minReplicas: 5
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
```

### Vertical Scaling

```python
# config/sota_config.yaml
execution:
  pool_size: 32  # Scale up worker processes
  
agents:
  high_compute_agent:
    resources:
      cpu: "4"
      memory: "8Gi"
```

---

## âœ… Production Best Practices

### 1. **Resource Limits**
```yaml
resources:
  requests:
    memory: "512Mi"
    cpu: "500m"
  limits:
    memory: "2Gi"
    cpu: "2000m"
```

### 2. **Health Checks**
```python
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "version": "1.0.0",
        "agents_loaded": len(router.agents)
    }
```

### 3. **Graceful Shutdown**
```python
import signal

def handle_shutdown(signum, frame):
    logger.info("Shutting down gracefully...")
    router.cleanup()
    sys.exit(0)

signal.signal(signal.SIGTERM, handle_shutdown)
```

### 4. **Rate Limiting**
```python
from slowapi import Limiter

limiter = Limiter(key_func=get_remote_address)

@app.post("/execute")
@limiter.limit("100/minute")
async def execute_agent(request):
    pass
```

### 5. **Circuit Breaker**
```python
from circuitbreaker import circuit

@circuit(failure_threshold=5, recovery_timeout=60)
async def call_databricks():
    # Automatically fails fast if Databricks is down
    pass
```

---

## ðŸŽ¯ Quick Deployment Commands

```bash
# Docker
docker build -t my-agent:v1 . && docker run -p 8000:8000 my-agent:v1

# Docker Compose
docker-compose up -d

# Kubernetes
kubectl apply -f k8s/ -n production

# Databricks
databricks fs cp dist/*.whl dbfs:/wheels/ && databricks jobs run-now --job-id 123

# AWS Lambda
serverless deploy --stage production

# Google Cloud
gcloud run deploy my-agent --image gcr.io/project/my-agent:v1
```

---

## ðŸ“š Next Steps

1. **[Configure monitoring](../monitoring/)** - Set up telemetry and metrics
2. **[Set up CI/CD](#cicd-pipelines)** - Automate deployments
3. **[Review security](#pre-deployment-checklist)** - Secure your deployment
4. **[Plan scaling](#scaling-strategy)** - Configure auto-scaling
5. **[Test in staging](#-pre-deployment-checklist)** - Validate before production

---

**Ready to deploy?** Choose your deployment option above and follow the guide! ðŸš€

