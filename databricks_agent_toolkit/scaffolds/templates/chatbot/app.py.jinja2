"""
{{ name }} - Chatbot Web App
Generated by: Databricks Agent Toolkit
"""

from flask import Flask, request, jsonify, render_template_string, Response
from databricks_agent_toolkit.integrations import DatabricksLLM
import mlflow
import os
import yaml
import asyncio
from pathlib import Path

# Load config
config = yaml.safe_load((Path(__file__).parent / "config.yaml").read_text())

# Initialize
app = Flask(__name__)
PORT = int(os.getenv('DATABRICKS_APP_PORT', 8000))
STREAMING = config["model"].get("streaming", False)
TOKEN_DELAY_MS = config["model"].get("token_delay_ms", 30)

llm = DatabricksLLM(
    endpoint=config["model"]["endpoint"],
    auto_trace=config["mlflow"].get("auto_trace", True)
)

# Setup MLflow
mlflow.set_tracking_uri("databricks")
try:
    mlflow.set_experiment(config["mlflow"]["experiment"])
    print(f"üìä MLflow: {config['mlflow']['experiment']}")
except Exception as e:
    print(f"‚ö†Ô∏è  MLflow: {e}")

print(f"üîÑ Streaming: {'enabled' if STREAMING else 'disabled'}")

# Chat UI
CHAT_HTML = '''<!DOCTYPE html>
<html>
<head>
    <title>{{ name }}</title>
    <style>
        body { font-family: -apple-system, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5; }
        h1 { color: #FF3621; margin-bottom: 5px; }
        .subtitle { color: #666; margin-bottom: 20px; }
        #chat-container { background: white; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); overflow: hidden; }
        #chat-box { height: 500px; overflow-y: auto; padding: 20px; display: flex; flex-direction: column; gap: 12px; }
        .bubble { padding: 10px 14px; border-radius: 18px; max-width: 70%; line-height: 1.5; word-wrap: break-word; }
        .user { align-self: flex-end; background: #007AFF; color: white; }
        .assistant { align-self: flex-start; background: #f1f1f1; color: #333; }
        .loading { color: #999; font-style: italic; }
        #input-container { display: flex; gap: 10px; padding: 20px; background: #f9f9f9; border-top: 1px solid #e0e0e0; }
        #input-box { flex: 1; padding: 12px 16px; border: 1px solid #ddd; border-radius: 20px; outline: none; }
        #input-box:focus { border-color: #007AFF; }
        #send-btn { padding: 12px 24px; background: #007AFF; color: white; border: none; border-radius: 20px; cursor: pointer; }
        #send-btn:hover { background: #0051D5; }
        #send-btn:disabled { background: #ccc; cursor: not-allowed; }
    </style>
</head>
<body>
    <h1>ü§ñ {{ name }}</h1>
    <p class="subtitle">Powered by <strong>{{ model }}</strong></p>
    <div id="chat-container">
        <div id="chat-box"></div>
        <div id="input-container">
            <input type="text" id="input-box" placeholder="Type your message..." onkeypress="if(event.key==='Enter') sendMessage()">
            <button id="send-btn" onclick="sendMessage()">Send</button>
        </div>
    </div>
    <script>
        const STREAMING = {% raw %}{{ streaming_enabled|lower }}{% endraw %};
        const TOKEN_DELAY_MS = {% raw %}{{ token_delay_ms }}{% endraw %};  // Configurable in config.yaml

        function sendMessage() {
            const input = document.getElementById('input-box');
            const sendBtn = document.getElementById('send-btn');
            const message = input.value.trim();
            if (!message) return;

            addBubble('user', message);
            input.value = '';
            sendBtn.disabled = true;

            if (STREAMING) {
                const bubble = addBubble('assistant', '');
                const tokenQueue = [];
                let isProcessing = false;
                let isDone = false;

                // Process queue with delay (typewriter effect)
                function processQueue() {
                    if (isProcessing) return;
                    isProcessing = true;

                    const interval = setInterval(() => {
                        if (tokenQueue.length > 0) {
                            bubble.textContent += tokenQueue.shift();
                            document.getElementById('chat-box').scrollTop = 999999;
                        } else if (isDone) {
                            clearInterval(interval);
                            sendBtn.disabled = false;
                        }
                    }, TOKEN_DELAY_MS);
                }

                const eventSource = new EventSource('/api/chat/stream?message=' + encodeURIComponent(message));

                eventSource.onmessage = (e) => {
                    // Split into individual characters for smooth rendering
                    for (const char of e.data) {
                        tokenQueue.push(char);
                    }
                    processQueue();
                };

                eventSource.addEventListener('done', () => {
                    isDone = true;
                    eventSource.close();
                });

                eventSource.onerror = () => {
                    if (!bubble.textContent) bubble.textContent = 'Error connecting';
                    eventSource.close();
                    sendBtn.disabled = false;
                };
            } else {
                const loadingId = addBubble('assistant', 'Thinking...', 'loading').id;

                fetch('/api/chat', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({message})
                })
                .then(r => r.json())
                .then(data => {
                    document.getElementById(loadingId).remove();
                    addBubble('assistant', data.error || data.response);
                    sendBtn.disabled = false;
                })
                .catch(err => {
                    document.getElementById(loadingId).remove();
                    addBubble('assistant', 'Error: ' + err);
                    sendBtn.disabled = false;
                });
            }
        }

        function addBubble(role, content, extraClass = '') {
            const chatBox = document.getElementById('chat-box');
            const bubble = document.createElement('div');
            bubble.id = 'bubble-' + Date.now();
            bubble.className = 'bubble ' + role + (extraClass ? ' ' + extraClass : '');
            bubble.textContent = content;
            chatBox.appendChild(bubble);
            chatBox.scrollTop = chatBox.scrollHeight;
            return bubble;
        }
    </script>
</body>
</html>'''

@app.route('/')
def home():
    return render_template_string(
        CHAT_HTML,
        name=config.get("agent_name", "{{ name }}"),
        model=config["model"]["endpoint"],
        streaming_enabled=STREAMING,
        token_delay_ms=TOKEN_DELAY_MS
    )

@app.route('/health')
def health():
    return jsonify({"status": "healthy", "model": config["model"]["endpoint"], "port": PORT})

@app.route('/api/chat', methods=['POST'])
@mlflow.trace(name="chatbot", span_type="CHAIN")
def chat():
    """Non-streaming endpoint"""
    try:
        user_message = request.get_json().get('message', '')
        if not user_message:
            return jsonify({"error": "No message"}), 400

        response = asyncio.run(llm.chat(
            messages=[
                {"role": "system", "content": "You are a helpful AI assistant."},
                {"role": "user", "content": user_message}
            ],
            temperature=config["model"].get("temperature", 0.7),
            max_tokens=config["model"].get("max_tokens", 500)
        ))

        return jsonify({"response": response["content"]})
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/chat/stream')
@mlflow.trace(name="chatbot_stream", span_type="CHAIN")
def chat_stream():
    """Streaming endpoint (Server-Sent Events)"""
    user_message = request.args.get('message', '')
    if not user_message:
        return jsonify({"error": "No message"}), 400

    def generate():
        async def async_stream():
            try:
                async for chunk in llm.stream(
                    messages=[
                        {"role": "system", "content": "You are a helpful AI assistant."},
                        {"role": "user", "content": user_message}
                    ],
                    temperature=config["model"].get("temperature", 0.7),
                    max_tokens=config["model"].get("max_tokens", 500)
                ):
                    if chunk and chunk.strip():
                        yield f"data: {chunk}\n\n"
                yield "event: done\ndata: \n\n"
            except Exception as e:
                print(f"‚ùå Streaming error: {e}")
                yield f"data: Error: {str(e)}\n\n"
                yield "event: done\ndata: \n\n"

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            async_gen = async_stream()
            while True:
                try:
                    chunk = loop.run_until_complete(async_gen.__anext__())
                    yield chunk
                except StopAsyncIteration:
                    break
        finally:
            loop.close()

    return Response(generate(), mimetype='text/event-stream')

if __name__ == '__main__':
    print(f"üöÄ Starting {{ name }} on port {PORT}")
    app.run(host='0.0.0.0', port=PORT, debug=False)
