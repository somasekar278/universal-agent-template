"""
{{ name }} - Assistant Agent with Memory (OpenAI API Compatible)
Generated by: Databricks Agent Toolkit v0.2.3

Uses MLflow AgentServer with @invoke/@stream decorators + conversation memory.
"""
import yaml
import uuid
import asyncio
from pathlib import Path
from typing import List, Dict, Optional
import mlflow
from mlflow.deployments import get_deploy_client
from mlflow.genai.agent_server.server import invoke, stream
from mlflow.types.responses import ResponsesAgentRequest, ResponsesAgentResponse
from memory_manager import MemoryManager

# Load config
config = yaml.safe_load((Path(__file__).parent / "config.yaml").read_text())

# Setup MLflow
mlflow.set_tracking_uri("databricks")
try:
    mlflow.set_experiment(config["mlflow"]["experiment"])
    print(f"ðŸ“Š MLflow: {config['mlflow']['experiment']}")
except Exception as e:
    print(f"âš ï¸  MLflow setup: {e}")

# Initialize LLM client
llm_client = get_deploy_client("databricks")
endpoint = config["model"]["endpoint"]
temperature = config["model"].get("temperature", 0.7)
max_tokens = config["model"].get("max_tokens", 500)
system_prompt = config["model"].get("system_prompt", "You are a helpful assistant with conversation memory.")
stream_delay_ms = config["model"].get("token_delay_ms", 50) / 1000.0

# Initialize memory manager
memory = MemoryManager(
    host=config["memory"].get("host"),
    database=config["memory"].get("database"),
    user=config["memory"].get("user"),
    password=config["memory"].get("password"),
    max_history=config["memory"].get("max_history", 20)
)
memory.initialize()

print(f"ðŸ¤– Agent initialized: {endpoint}")
print(f"ðŸ’¾ Memory: Lakebase")


def _extract_session_id(request: dict) -> str:
    """Extract or generate session_id from request"""
    return request.get("session_id", str(uuid.uuid4()))


def _extract_user_message(messages: List[dict]) -> Optional[str]:
    """Extract the last user message from messages list"""
    for msg in reversed(messages):
        if msg.get("role") == "user":
            return msg.get("content", "")
    return None


def _build_messages_with_history(session_id: str) -> List[Dict]:
    """Build messages with system prompt + conversation history"""
    history = memory.get_messages_for_llm(session_id)
    return [{"role": "system", "content": system_prompt}] + history


{% if streaming %}
@stream()
async def stream_handler(request: dict):
    """Streaming invocation handler with conversation memory."""
    # Extract session ID
    session_id = _extract_session_id(request)

    # Extract and store user message
    input_messages = request.get("input", [])
    user_message = _extract_user_message(input_messages)
    if user_message:
        memory.store_message(session_id, "user", user_message)

    # Build messages with history
    messages = _build_messages_with_history(session_id)

    # Call LLM (non-streaming, then simulate streaming for demo)
    response = llm_client.predict(
        endpoint=endpoint,
        inputs={"messages": messages, "temperature": temperature, "max_tokens": max_tokens}
    )
    content = response["choices"][0]["message"]["content"]

    # Stream with configurable delay
    full_response = []
    for word in content.split():
        chunk = word + " "
        full_response.append(chunk)
        yield {"role": "assistant", "content": chunk, "session_id": session_id}
        await asyncio.sleep(stream_delay_ms)

    # Store complete assistant response in memory (after streaming completes)
    memory.store_message(session_id, "assistant", "".join(full_response).strip())
{% else %}
@invoke()
def invoke_handler(request) -> ResponsesAgentResponse:
    """Non-streaming invocation handler with conversation memory."""
    # Extract messages (handles both dict and ResponsesAgentRequest)
    if isinstance(request, dict):
        input_messages = request.get("input", [])
        session_id = request.get("session_id", str(uuid.uuid4()))
    else:
        input_messages = [msg.model_dump() for msg in request.input]
        session_id = request.model_dump().get("session_id", str(uuid.uuid4()))

    # Extract and store user message
    user_message = _extract_user_message(input_messages)
    if user_message:
        memory.store_message(session_id, "user", user_message)

    # Build messages with history
    messages = _build_messages_with_history(session_id)

    # Call LLM
    response = llm_client.predict(
        endpoint=endpoint,
        inputs={"messages": messages, "temperature": temperature, "max_tokens": max_tokens}
    )

    # Extract response content
    content = response["choices"][0]["message"]["content"]

    # Store assistant response in memory
    memory.store_message(session_id, "assistant", content)

    # Return ResponsesAgent format
    return ResponsesAgentResponse(
        output=[{
            "id": str(uuid.uuid4()),
            "type": "message",
            "role": "assistant",
            "content": [{"type": "output_text", "text": content}]
        }]
    )
{% endif %}
