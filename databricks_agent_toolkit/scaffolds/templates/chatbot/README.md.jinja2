# {{ name }} - L1 Simple Chatbot

**Generated by:** Databricks Agent Toolkit v0.1.0
**Learning Level:** Beginner (2-4 hours)

## ğŸ¯ What You'll Learn

- âœ… Use Databricks Model Serving via toolkit
- âœ… Automatic MLflow tracing
- âœ… Basic conversation handling
- âœ… Deploy to Databricks Apps

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Set Up Authentication

The toolkit automatically uses Databricks authentication. Make sure you have one of:

**Option A: Environment Variables**
```bash
export DATABRICKS_HOST="https://your-workspace.cloud.databricks.com"
export DATABRICKS_TOKEN="dapi123..."
```

**Option B: Databricks CLI**
```bash
databricks auth login --host https://your-workspace.cloud.databricks.com
```

### 3. Run Locally

```bash
python chatbot.py
```

### 4. Deploy to Databricks Apps

**Pure Databricks Asset Bundles (DABs) workflow:**

1. **Authenticate:**
```bash
databricks auth login --host https://your-workspace.cloud.databricks.com
```

2. **Validate the bundle:**
```bash
databricks bundle validate
```

3. **Deploy the bundle:**
```bash
databricks bundle deploy
```

4. **Start the app:**
```bash
databricks bundle run {{ name }}
```

5. **Add Model Serving permission (one-time setup):**

   After first deployment, add Model Serving access:
   - Go to **Databricks Workspace** â†’ **Apps** â†’ **{{ name }}**
   - Click **Resources** â†’ **Add Resource**
   - Add **Model Serving** with **Can query** permission
   - Then start: `databricks bundle run {{ name }}`

6. **Access your app:**
   `https://your-workspace.cloud.databricks.com/ml/apps/{{ name }}`

**Deploy to production:**
```bash
databricks bundle deploy -t prod
databricks bundle run {{ name }} -t prod
```

**Local development (optional):**
```bash
databricks apps run-local --prepare-environment --debug
# Navigate to http://localhost:8001
```

## âš¡ Streaming vs. Non-Streaming

The chatbot supports **two response modes**, configured in `config.yaml`:

### **Non-Streaming (Default)**
```yaml
model:
  streaming: false
```
- **Behavior:** Full response appears at once after thinking
- **Use when:** Response speed is less critical, simpler deployment
- **User experience:** "Thinking..." â†’ Complete response

### **Streaming (Server-Sent Events)**
```yaml
model:
  streaming: true
```
- **Behavior:** Tokens appear word-by-word as they're generated
- **Use when:** You want interactive, ChatGPT-like experience
- **User experience:** Real-time response streaming
- **Technical:** Uses SSE (`text/event-stream`) with `EventSource` JavaScript API

**To switch modes:** Change `streaming: true/false` in `config.yaml` and redeploy.

## ğŸ“ What to Customize

### **1. System Prompt** (chatbot.py, line 23)

```python
content: "You are a helpful assistant."  # Change this
```

**Try:**
- "You are a customer support agent for Acme Corp."
- "You are a friendly tutor helping students learn math."
- "You are a technical assistant specializing in Python."

### **2. Temperature** (chatbot.py, line 29)

```python
temperature=0.7  # 0.0 = deterministic, 1.0 = creative
```

**Guidelines:**
- `0.0-0.3` - Factual, consistent responses (customer support, technical docs)
- `0.4-0.7` - Balanced (general chatbot, tutoring)
- `0.8-1.0` - Creative, varied responses (creative writing, brainstorming)

### **3. Model** (config.yaml)

```yaml
endpoint: {{ model }}  # Try different models
```

**Available models:**
- `databricks-claude-sonnet-4-5` - High quality, recommended â­
- `databricks-gpt-5` - Latest GPT model
- `databricks-gemini-2-5-pro` - Google's latest
- `databricks-llama-4-maverick` - Meta's latest open model

### **4. Response Mode** (chatbot.py, line 6 + config.yaml)

**Batch Mode (Default):**
```python
ENABLE_STREAMING = False  # Get complete response at once
```

**Use when:**
- âœ… You need the full response before displaying
- âœ… Processing the response (parsing, validation)
- âœ… Storing complete responses in logs

**Streaming Mode:**
```python
ENABLE_STREAMING = True  # Get tokens as they're generated
```

**Use when:**
- âœ… Real-time user experience (like ChatGPT)
- âœ… Long responses (show progress)
- âœ… Interactive applications

**How to switch:**

**Option A: Edit code** (chatbot.py)
```python
ENABLE_STREAMING = True  # Change to True
```

**Option B: Edit config** (config.yaml)
```yaml
model:
  streaming: true  # Change to true
```

**Try both!** Run the chatbot and compare the experience.

## ğŸ“ What's Happening Under the Hood

### **Toolkit Integration:**

```python
from databricks_agent_toolkit.integrations import DatabricksLLM
llm = DatabricksLLM(endpoint="{{ model }}")
```

**The toolkit handles:**
- âœ… Authentication via Databricks SDK
- âœ… Automatic MLflow tracing (`@mlflow.trace`)
- âœ… Token usage tracking
- âœ… Error handling and retries

**Without toolkit, you'd need:**
```python
# Manual setup (what the toolkit does for you)
from databricks.sdk import WorkspaceClient
import mlflow

w = WorkspaceClient()
client = w.serving_endpoints.get_open_ai_client()
mlflow.set_tracking_uri("databricks")
# ... plus error handling, tracing setup, etc.
```

## ğŸ“Š View Your Traces & Provide Feedback

All conversations are automatically logged to MLflow for analysis.

**End of conversation:**
```
============================================================
ğŸ‘‹ Thanks for chatting!
   3 conversation(s) logged for analysis
============================================================
```

**For developers** - Enable `show_url` in config to see MLflow links:
```yaml
mlflow:
  show_url: true  # Shows technical MLflow URLs
```

### **What's Logged to MLflow:**

1. **Traces** - Every conversation turn logged
2. **Metrics** - Token usage, response times
3. **Parameters** - Model settings, temperature
4. **Assessments** - Your feedback ratings â­

### **Assessments (Human Feedback)**

The chatbot periodically asks for feedback:

```
ğŸ“Š Quick Feedback (helps improve the bot)
==================================================
Rate this conversation (1-5, or press Enter to skip): 5
Any comments? (optional): Great response!
âœ… Feedback recorded - thank you!
```

**Configuration** (`config.yaml`):
```yaml
mlflow:
  assessments:
    enabled: true  # Set to false to disable
    frequency: 5   # Ask every N conversations (0 = never)
```

**Why This Matters:**
- âœ… Creates labeled dataset for evaluation
- âœ… Identifies edge cases & problems
- âœ… Triggers optimization when quality drops
- âœ… Provides ground truth for scorer tuning

**View Assessments in MLflow:**
1. Click on your conversation run
2. Click the **"Assessments"** tab
3. See all ratings and comments
4. Use for dataset curation & optimization

## ğŸ”„ Continuous Improvement Cycle

Your chatbot is part of MLflow's continuous improvement workflow:

```
User Conversations â†’ Traces â†’ Assessments â†’ Dataset â†’ Evaluation â†’ Optimization â†’ Deploy
                      â†‘                                                              â†“
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. **Conversations logged** - Every interaction traced
2. **Assessments collected** - Periodic feedback (every 5 turns)
3. **Dataset curated** - High/low-rated examples â†’ training data
4. **Evaluation** - Test against labeled dataset
5. **Optimization** - DSPy/TextGrad improve prompts
6. **Deploy** - Updated bot goes live
7. **Repeat** - Continuous monitoring & improvement

**This is all built-in!** Just keep chatting and providing feedback.

## â¡ï¸ Next Steps

### **Ready for assistant (with memory)?**

Add conversation memory and context:

```bash
databricks-agent-toolkit generate assistant {{ name }}-with-memory
```

**Key differences:**
- âœ… Stores conversation history in Lakebase
- âœ… Retrieves context from past interactions
- âœ… Session management
- âœ… Delta Lake storage

### **Want to Learn More?**

- [Databricks Agent Toolkit Docs](https://databricks-agent-toolkit.readthedocs.io)
- [Databricks Model Serving](https://docs.databricks.com/machine-learning/model-serving/)
- [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/)

---

**Generated with â¤ï¸ by Databricks Agent Toolkit**
