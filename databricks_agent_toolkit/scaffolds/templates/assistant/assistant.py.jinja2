"""{{ name }} - Context-Aware Assistant (L2) with Lakebase memory"""
import os, sys, asyncio, uuid, yaml
from typing import List, Dict, Any

try:
    from databricks_agent_toolkit.integrations import DatabricksLLM
    import mlflow
except ImportError:
    print("âŒ Install: pip install databricks-agent-toolkit")
    sys.exit(1)

from memory_manager import MemoryManager

class Assistant:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.llm = DatabricksLLM(
            endpoint=config["model"]["endpoint"],
            auto_trace=config.get("mlflow", {}).get("auto_trace", True)
        )
        self.memory = MemoryManager(
            host=config["memory"].get("host"),
            database=config["memory"].get("database"),
            user=config["memory"].get("user"),
            password=config["memory"].get("password"),
            max_history=config["memory"].get("max_history", 20)
        )
        self.memory.initialize()
        if config.get("mlflow", {}).get("auto_trace"):
            mlflow.set_experiment(config["mlflow"].get("experiment", "/Shared/{{ name }}"))

    async def chat(self, session_id: str, user_message: str) -> str:
        self.memory.store_message(session_id, "user", user_message)
        history = self.memory.get_messages_for_llm(session_id)
        messages = [
            {"role": "system", "content": self.config["model"].get("system_prompt", "You are a helpful assistant.")},
            *history
        ]

        if self.config["model"].get("streaming", False):
            full_response = ""
            async for chunk in self.llm.stream(messages):
                content = chunk.get("content", "")
                print(content, end="", flush=True)
                full_response += content
            print()
            response = full_response
        else:
            result = await self.llm.chat(messages)
            response = result["content"]

        self.memory.store_message(session_id, "assistant", response)
        return response

    def clear_session(self, session_id: str):
        self.memory.clear_conversation(session_id)

    def close(self):
        self.memory.close()

async def main():
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   {{ name }} - Context-Aware Assistant (L2)              â•‘
    â•‘   Powered by Databricks + Lakebase Memory                â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    Type 'exit' to quit, 'clear' to clear history.
    """)

    with open("config.yaml") as f:
        config = yaml.safe_load(f)

    assistant = Assistant(config)
    session_id = os.getenv("SESSION_ID", str(uuid.uuid4()))

    print(f"ğŸ“ Session: {session_id}")
    print(f"ğŸ¤– Model: {config['model']['endpoint']}")
    print(f"ğŸ’¾ Memory: Lakebase\n")

    if config.get("mlflow", {}).get("auto_trace"):
        mlflow.start_run()

    try:
        while True:
            user_input = input("You: ").strip()
            if not user_input:
                continue
            if user_input.lower() == "exit":
                print("\nğŸ‘‹ Goodbye!")
                break
            if user_input.lower() == "clear":
                assistant.clear_session(session_id)
                print("âœ… History cleared\n")
                continue

            print("Assistant: ", end="" if config["model"].get("streaming") else "")
            response = await assistant.chat(session_id, user_input)
            if not config["model"].get("streaming"):
                print(response)
            print()
    finally:
        assistant.close()
        if config.get("mlflow", {}).get("auto_trace"):
            mlflow.end_run()

if __name__ == "__main__":
    asyncio.run(main())
