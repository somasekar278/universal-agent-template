# {{ name }} Configuration (L2 - Assistant)
#
# Context-aware assistant with conversation memory.

type: assistant
name: {{ name }}

# Model Configuration
model:
  endpoint: {{ model | default("databricks-claude-sonnet-4-5") }}
  streaming: false  # Set to true for token-by-token streaming
  temperature: 0.7  # Controls randomness (0.0-1.0)
  max_tokens: 500  # Maximum tokens in response
  system_prompt: "You are a helpful AI assistant."

# Memory Configuration (Lakebase - PostgreSQL)
memory:
  # Connection details (or set environment variables)
  host: null  # Set LAKEBASE_HOST env var
  database: null  # Set LAKEBASE_DATABASE env var
  user: null  # Set LAKEBASE_USER env var
  password: null  # Set LAKEBASE_PASSWORD env var

  # Memory settings
  max_history: 20  # Maximum number of messages to keep in context

# RAG Configuration (Optional - for knowledge retrieval)
rag:
  enabled: {{ enable_rag | default(false) | lower }}  # Set to true to enable RAG

  # Document source (UC Volume path or local path for testing)
  # UC Volume example: /Volumes/main/default/agent_docs
  # Local path example: /path/to/docs
  source: {{ rag_source | default('null') if rag_source else 'null' }}

  # Backend: pgvector (default, cost-effective) or vector_search (enterprise-scale)
  backend: {{ rag_backend | default('pgvector') }}

  # Retrieval settings
  top_k: 5  # Number of documents to retrieve
  chunk_size: 500  # Characters per chunk

  # Embedding model
  embedding_model: databricks-bge-large-en

  # pgvector index type (only for pgvector backend)
  # Options: ivfflat (faster indexing, good for updates) or hnsw (better recall, read-heavy)
  index_type: {{ index_type | default('ivfflat') }}  # Default: ivfflat

  # Vector Search config (only if backend: vector_search)
  vector_search_endpoint: {{ vector_search_endpoint | default('one-env-shared-endpoint-0') }}  # Your Vector Search endpoint name
  index_name: {{ vector_search_index | default('null') if vector_search_index else 'null' }}  # e.g., main.default.docs_index (catalog.schema.index_name)

# MLflow Configuration
mlflow:
  auto_trace: true
  experiment: /Shared/{{ name }}
  show_url: false  # Set to true to see MLflow trace URLs (developer mode)

  # Human feedback collection
  assessments:
    enabled: true
    frequency: 5  # Collect feedback every N turns

# Databricks Apps Deployment
deployment:
  name: {{ name }}
  port: 8000
  resources:
    cpu: 2
    memory: 4Gi
